{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Markov Chain Monte Carlo\n",
    " \n",
    "This Jupyter notebook is the Python equivalent of the R code in section 12.4 R, pp. 512 - 517, [Introduction to Probability, 1st Edition](https://www.crcpress.com/Introduction-to-Probability/Blitzstein-Hwang/p/book/9781466575578), Blitzstein & Hwang.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis-Hastings\n",
    "\n",
    "Here's how to implement the Metropolis-Hastings algorithm for Example 12.1.8, the Normal-Normal model. First, we choose our observed value of $Y$ and decide on values for the constants $\\sigma$, $\\mu$, and $\\tau$:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y <- 3\n",
    "sigma <- 1\n",
    "mu <- 0\n",
    "tau <- 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to choose the standard deviation of the proposals for step 1 of the algorithm, as explained in Example 12.1.8; for this problem, we let $d = 1$. We set the number of iterations to run, and we allocate a vector `theta` of length 10<sup>4</sup> which we will fill with our simulated draws:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d <- 1\n",
    "niter <- 10^4\n",
    "theta <- rep(0,niter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the main loop. We initialize $\\theta$ to the observed value $y$, then run the algorithm described in Example 12.1.8:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "theta[1] <- y\n",
    "for (i in 2:niter){\n",
    "    theta.p <- theta[i-1] + rnorm(1,0,d)\n",
    "    r <- dnorm(y,theta.p,sigma) * dnorm(theta.p,mu,tau) / \n",
    "           (dnorm(y,theta[i-1],sigma) * dnorm(theta[i-1],mu,tau))\n",
    "    flip <- rbinom(1,1,min(r,1))\n",
    "    theta[i] <- if(flip==1) theta.p else theta[i-1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's step through each line inside the loop. The proposed value of $\\theta$ is `theta.p`, which equals the previous value of $\\theta$ plus a Normal random variable with mean 0 and standard deviation $d$ (recall that rnorm takes the standard deviation and not the variance as input). The ratio r is\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{f_{\\theta|Y}(x^{\\prime}|y)}{f_{\\theta|Y}(x|y)} &= \\frac{e^{-\\frac{1}{2 \\, \\sigma^2}(y-x^{\\prime})^2} \\,\\, e^{-\\frac{1}{2 \\, \\tau^2}(x^{\\prime}-\\mu)^2}}{e^{-\\frac{1}{2 \\, \\sigma^2}(y-x)^2} \\,\\, e^{-\\frac{1}{2 \\, \\tau^2}(x-\\mu)^2}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where theta.p is playing the role of $x^{\\prime}$ and theta[i-1] is playing the role of $x$. The coin flip to determine whether to accept or reject the proposal is flip, which is a coin flip with probability min(r,1) of Heads (encoding Heads as 1 and Tails as 0). Finally, we set theta[i] equal to the proposed value if the coin flip lands Heads, and keep it at the previous value otherwise.\n",
    "\n",
    "The vector theta now contains all of our simulation draws. We typically discard some of the initial draws to give the chain some time to approach the stationary distribution. The following line of code discards the first half of the draws:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "theta <- theta[-(1:(niter/2))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what the remaining draws look like, we can create a histogram using hist(theta). We can also compute summary statistics such as mean(theta) and var(theta), which give us the sample mean and sample variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs\n",
    "\n",
    "Now let's implement Gibbs sampling for Example 12.2.6, the chicken-egg problem with unknown hatching probability and invisible unhatched eggs. The first step is to decide on our observed value of $X$, as well as the constants $\\lambda$, $a$, $b$:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x <- 7 \n",
    "lambda <- 10 \n",
    "a <- 1 \n",
    "b <- 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we decide how many iterations to run, and we allocate space for our results, creating two vectors `p` and `N` of length 10<sup>4</sup> which we will fill with our simulated draws:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "niter <- 10^4 \n",
    "p <- rep(0,niter) \n",
    "N <- rep(0,niter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're ready to run the Gibbs sampler. We initialize `p` and `N` to the values 0.5 and $2x$, respectively, and then we run the algorithm as explained in Example 12.2.6:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p[1] <- 0.5 \n",
    "N[1] <- 2*x \n",
    "for (i in 2:niter){ \n",
    "    p[i] <- rbeta(1,x+a,N[i-1]-x+b)\n",
    "    N[i] <- x + rpois(1,lambda*(1-p[i-1])) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we discard the initial draws:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p <- p[-(1:(niter/2))] \n",
    "N <- N[-1:(niter/2))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what the remaining draws look like, we can make histograms using hist(p) and hist(N), which is how we created Figure 12.5. We can also compute summary statistics such as mean(p) or median(p)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "&copy; Blitzstein, Joseph K.; Hwang, Jessica. Introduction to Probability (Chapman & Hall/CRC Texts in Statistical Science)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
